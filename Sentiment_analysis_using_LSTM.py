# -*- coding: utf-8 -*-
"""test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Da43TUXCMRe8JHmOO53MkjbDrp7G7Kqc
"""

# Commented out IPython magic to ensure Python compatibility.
#@title defalt
from google.colab import drive
drive.mount('/content/drive')

# Setting toolkit folder as working directory

# %cd /content/drive/My Drive/Project8_SentimentAnalysis_with_NeuralNetwork
! ls

# Importing essential libraries and functions

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from numpy import array

from keras.preprocessing.text import one_hot, Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers.core import Activation, Dropout, Dense
from keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM
from sklearn.model_selection import train_test_split

#@title Importing IMDb Movie Reviews dataset

movie_reviews = pd.read_csv("a1_IMDB_Dataset.csv")

# Dataset exploration

movie_reviews.shape

movie_reviews.head(5)

# Checking for missing values

movie_reviews.isnull().values.any()

# Let's observe distribution of positive / negative sentiments in dataset

import seaborn as sns
sns.countplot(x='sentiment', data=movie_reviews)

#Data Preprocessing

movie_reviews["review"][2]

# You can see that our text contains punctuations, brackets, HTML tags and numbers 
# We will preprocess this text in the next section

TAG_RE = re.compile(r'<[^>]+>')

def remove_tags(text):
    '''Removes HTML tags: replaces anything between opening and closing <> with empty space'''

    return TAG_RE.sub('', text)

import nltk
nltk.download('stopwords')

def preprocess_text(sen):
    '''Cleans text data up, leaving only 2 or more char long non-stepwords composed of A-Z & a-z only
    in lowercase'''
    
    sentence = sen.lower()

    # Remove html tags
    sentence = remove_tags(sentence)

    # Remove punctuations and numbers
    sentence = re.sub('[^a-zA-Z]', ' ', sentence)

    # Single character removal
    sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)  # When we remove apostrophe from the word "Mark's", the apostrophe is replaced by an empty space. Hence, we are left with single character "s" that we are removing here.

    # Remove multiple spaces
    sentence = re.sub(r'\s+', ' ', sentence)  # Next, we remove all the single characters and replace it by a space which creates multiple spaces in our text. Finally, we remove the multiple spaces from our text as well.

    # Remove Stopwords
    pattern = re.compile(r'\b(' + r'|'.join(stopwords.words('english')) + r')\b\s*')
    sentence = pattern.sub('', sentence)

    return sentence

# Calling preprocessing_text function on movie_reviews

X = []
sentences = list(movie_reviews['review'])
for sen in sentences:
    X.append(preprocess_text(sen))

# Sample cleaned up movie review 

X[2]

# As we shall use Word Embeddings, stemming/lemmatization is not performed as a preprocessing step here

# Converting sentiment labels to 0 & 1

y = movie_reviews['sentiment']

y = np.array(list(map(lambda x: 1 if x=="positive" else 0, y)))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

# The train set will be used to train our deep learning models 
# while test set will be used to evaluate how well our model performs 

#Preparing embedding layer

# Embedding layer expects the words to be in numeric form 
# Using Tokenizer function from keras.preprocessing.text library
# Method fit_on_text trains the tokenizer 
# Method texts_to_sequences converts sentences to their numeric form

word_tokenizer = Tokenizer()
word_tokenizer.fit_on_texts(X_train)

X_train = word_tokenizer.texts_to_sequences(X_train)
X_test = word_tokenizer.texts_to_sequences(X_test)

# Adding 1 to store dimensions for words for which no pretrained word embeddings exist

vocab_length = len(word_tokenizer.word_index) + 1

vocab_length

# Padding all reviews to fixed length 100

maxlen = 100

X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

# Load GloVe word embeddings and create an Embeddings Dictionary

from numpy import asarray
from numpy import zeros

embeddings_dictionary = dict()
glove_file = open('a2_glove.6B.100d.txt', encoding="utf8")

for line in glove_file:
    records = line.split()
    word = records[0]
    vector_dimensions = asarray(records[1:], dtype='float32')
    embeddings_dictionary [word] = vector_dimensions
glove_file.close()

# Create Embedding Matrix having 100 columns 
# Containing 100-dimensional GloVe word embeddings for all words in our corpus.

embedding_matrix = zeros((vocab_length, 100))
for word, index in word_tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

embedding_matrix.shape

#Recurrent Neural Network (LSTM)

from keras.layers import LSTM

# Neural Network architecture

lstm_model = Sequential()
embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)

lstm_model.add(embedding_layer)
lstm_model.add(LSTM(128))

lstm_model.add(Dense(1, activation='sigmoid'))

# Model compiling

lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
print(lstm_model.summary())

# Model Training

lstm_model_history = lstm_model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)

# Predictions on the Test Set

score = lstm_model.evaluate(X_test, y_test, verbose=1)

# Model Performance

print("Test Score:", score[0])
print("Test Accuracy:", score[1])

#@title Model Performance Charts

import matplotlib.pyplot as plt

plt.plot(lstm_model_history.history['acc'])
plt.plot(lstm_model_history.history['val_acc'])

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()

plt.plot(lstm_model_history.history['loss'])
plt.plot(lstm_model_history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='upper left')
plt.show()

lstm_model.save('/content/drive/MyDrive/Colab Notebooks/model.h5')

!pip install flask --quiet
#!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
#!unzip ngrok-stable-linux-amd64.zip
!pip install flask-ngrok --quiet
!pip install pyngrok --quiet
!ngrok authtoken 2PSRXJDXr8TPn9xXINDidSWH8S2_2A5PpTedKMmug7LEqpbPP
print("Completed!")

review = "This movie was amazing! The acting was superb and the storyline was captivating."
def predict_sentiment(review):
    # Preprocess the input review
    review = preprocess_text(review)
    # Tokenize the review and pad it to maxlen
    review = word_tokenizer.texts_to_sequences([review])
    review = pad_sequences(review, padding='post', maxlen=maxlen)
    # Make the prediction using the LSTM model
    prediction = lstm_model.predict(review)[0][0]
    # Return "Positive" if the predicted sentiment is greater than 0.5, else "Negative"
    if prediction > 0.5:
        return "Positive"
    else:
        return "Negative"
    predicted_sentiment = predict_sentiment(review)
    print("Predicted sentiment:", predicted_sentiment)

review = "Ladies and gentlemen, I stand before you today to express my utter disappointment and frustration with the current state of affairs. Our society has become a breeding ground for incompetence, corruption, and injustice. The values we once held dear seem to have been tossed aside in favor of selfishness and greed. "
predicted_sentiment = predict_sentiment(review)
print("Predicted sentiment:", predicted_sentiment)

review = "This movie was amazing! The acting was superb and the storyline was captivating."
predicted_sentiment = predict_sentiment(review)
print("Predicted sentiment:", predicted_sentiment)

def predict_sentiment(text):
    # Clean up text
    cleaned_text = preprocess_text(text)
    
    # Convert to numeric form using tokenizer
    sequence = word_tokenizer.texts_to_sequences([cleaned_text])
    padded_sequence = pad_sequences(sequence, padding='post', maxlen=maxlen)
    
    # Make prediction using trained model
    prediction = lstm_model.predict(padded_sequence)[0][0]
    
    # Return sentiment label
    if prediction > 0.5:
        return "Positive"
    else:
        return "Negative"

score = lstm_model.evaluate(X_test, y_test, verbose=1)
print("Test Score:", score[0])
print("Test Accuracy:", score[1])

def calculate_accuracy(model, input_text):
    # Preprocess the input text
    preprocessed_text = preprocess_text(input_text)
    # Convert the preprocessed text to its numeric form
    input_sequence = word_tokenizer.texts_to_sequences([preprocessed_text])
    # Pad the input sequence to the fixed length of 100
    padded_input_sequence = pad_sequences(input_sequence, padding='post', maxlen=maxlen)
    # Make predictions using the model
    predictions = model.predict(padded_input_sequence)
    # Convert the predictions to binary class labels
    binary_predictions = [1 if prediction >= 0.5 else 0 for prediction in predictions]
    # Calculate the accuracy
    accuracy = binary_predictions[0]
    return accuracy

input_text = "negative ."
accuracy = calculate_accuracy(lstm_model, input_text)
print("Accuracy:", accuracy)